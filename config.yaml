# ============ CONFIG ============

# ============ Paths ============
paths:
  raw: data/raw
  parquet: data/parquet
  geo: data/geo
  figures: output/figures

# -------------------------
# 01 – INGEST
# -------------------------
ingest:
  format: geolife
  input_dir: "data/raw/Geolife Trajectories 1.3/Data"

  # Write mode (choose exactly one of the following two flags)
  write_partitioned_by_user: false   # true => data/parquet/by_user/user_id=<id>/*.parquet
  write_partitioned_by_traj: false   # true => data/parquet/by_traj/traj_id=<id>/*.parquet

  # Micro-batching for the monolithic Parquet file
  rows_per_flush: 250000

  # Parallelism
  max_workers: 0                     # 0 = auto (all available CPU cores)

  # Time
  timezone_policy: "convert_from_local:Asia/Shanghai"  # Geolife e local Beijing; convert to UTC
  valid_time:
    min: "2007-04-01"
    max: "2012-08-31"

  # Space
  bbox: [115.2, 39.4, 117.2, 41.0]                         # ex: [min_lon, min_lat, max_lon, max_lat]; null = keep all

  # Logging
  log_file: "output/logs/ingest.log"
  log_level: "INFO"                           # DEBUG / INFO / WARNING / ERROR
  log_rotation: "50 MB"                       # sau "daily"
  log_retention: "14 days"
  log_compression: "zip"

  # Optional processing toggles
  processing:
    deduplicate: true

# -------------------------
# 02 – REDUCE
# -------------------------
reduce:
  # Keep the first point in each fixed time bucket (per user and trajectory).
  # Examples: "1s", "2s", "5s", "10s"
  time_bucket: "2s"

  # Drop transitions implying speeds above this threshold (km/h).
  max_speed_kmh: 300

  # Drop transitions that move less than this distance (metres).
  min_move_distance_m: 2

  # Maximum pause between consecutive points before considering the start of a new trajectory (minutes)
  max_time_gap_min: 30

  # Parameters for the Hampel filter (outlier detection)
  # If window=0, the filter is disabled
  hampel_window: 0
  hampel_nsigmas: 3

  log_file: "output/logs/reduce.log"   # file log
  log_level: "INFO"                    # DEBUG / INFO / WARNING / ERROR

  # optional – run only once for sensitivity analysis
  reduce_sensitivity:
    time_bucket: "2s"
    min_move_distance_m: 5
    max_speed_kmh: 300
    max_time_gap_min: 30
    hampel_window: 5            # ON just here
    hampel_nsigmas: 3
    log_file: "output/logs/reduce_sens_{time}.log"
    log_level: "INFO"

# -------------------------
# 03 – H3 ASSIGN
# -------------------------
h3_assign:
  # Input file (from step 02)
  input_file: "points_reduced.parquet"

  # Choose ONE of the following:
  # Single resolution (default 8 ≈ ~58 m average hex area edge ~0.53 km):
  resolution: 8            # ~58 m; (0=very coarse ... 15=very fine)
  output_file: "points_with_h3.parquet"         # Output file when using single resolution:
  # OR multiple resolutions for comparison:
  # resolutions: [7, 8, 9]
  # output_file_multi: "points_with_h3_r{res}.parquet"


  # Logging
  log_file: "output/logs/h3_assign.log"
  log_level: "INFO"   # DEBUG / INFO / WARNING / ERROR

# -------------------------
# 04 – AGGREGATE TEMPORAL
# -------------------------
aggregate:
  input_file: "points_with_h3.parquet"

  # Outputs
  output_users: "h3_hour_users.parquet"
  output_exposure: "h3_hour_person_minutes.parquet"
  output_dwell: "h3_hour_dwell_median.parquet"
  output_users_daytype: "h3_hour_users_daytype.parquet"

  # Methodology
  h3_resolution: 8
  dwell_same_cell_only: true
  per_interval_cap_s: 7200        # cap a single stay-interval to ≤ 2h (or null)
  per_user_cap_s: 3600            # cap per user per hour (seconds) or null
  presence_threshold_s: 300       # presence threshold for n_users
  presence_thresholds_extra: [ 60, 600 ]
  write_users_by_daytype: true
  tz_name: "Asia/Shanghai"
  # max_dwell_s: 900


  # Logging
  log_file: "output/logs/h3_aggregate.log"
  log_level: "INFO"

# Privacy (applies to all aggregated outputs in step 04)
privacy:
  min_count_per_cell_hour: 5   # drop groups with < 5 observed users

# -------------------------
# 05 – EXPORT GEOJSON
# -------------------------
export:
  # Any Parquet with 'h3_cell' (optionally 'h3_res'); usually from step 04 users file
  h3_cells_source: "data/parquet/h3_hour_users.parquet"

  # Outputs
  out_polygons: "data/geo/h3_polygons.geojson"
  out_frontier: "data/geo/h3_frontier.geojson"

   # Frontier options: union | convex_hull | concave_hull (concave needs 'alphashape')
  compute_frontier: true
  frontier_method: "union"        # union | concave_hull | convex_hull
  concavity: 2.0

  # Geometry simplification / size control
  simplify_tolerance_m: 0           # 0=off; try 2..10 for lighter files
  decimal_precision: 6              # coordinate rounding (lon/lat)

  # Optional spatial filter (keeps cells whose centroid is inside this bbox)
  bbox: null                        # [min_lon, min_lat, max_lon, max_lat] or null

# -------------------------
# 06 – KDE
# -------------------------
kde:
  # Input (requires: lon, lat, t[UTC]; optional: a weight column)
  input_file: "points_reduced.parquet"

  # Hours to estimate (UTC 0–23)
  hours: [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23]

  # Grid in EPSG:3857 (metric)
  pixel_size_m: 100                   # metres per pixel (target; may auto-adjust)
  max_pixels: 10000000                # safety cap: total pixels per raster
  max_side: 30000                    # safety cap: max pixels along an axis
  extent_bbox: null                  # [min_lon, min_lat, max_lon, max_lat]; null → from data

  # Bandwidth
  bw_rule: "fixed"                   # fixed | scott | silverman | cv
  bandwidth_m: 150                   # used iff bw_rule=fixed
  cv_candidates_m: [ 50, 75, 100, 150, 200, 300 ]

  # Temporal smoothing across neighbour hours (hour-of-day, circular)
  st_sigma_h: 0.0                    # 0 = off; e.g., 1.0 pulls from adjacent hours

  # Weights & correction
  weight_col: null                   # e.g., "dwell_s"; null → weight 1 per point
  edge_correction: true              # divide by Gaussian-smoothed support mask

  # Outputs
  out_parquet: "kde_hour.parquet"    # columns: {hour, x, y, value}, x/y in metres (EPSG:3857)

  # KDE in H3 (optional)
  h3_cells_source: "data/parquet/h3_hour_users.parquet"  # any Parquet with 'h3_cell'
  out_kde_in_h3: "kde_in_h3.parquet"
  kde_in_h3_method: "centroid"       # centroid | subsample
  subsample_factor: 4                # if method=subsample: samples per cell edge

  # Logging
  log_file: "output/logs/kde.log"
  log_level: "INFO"

# -------------------------
# 07 – ADS
# -------------------------
ads:
  # Inputs (from steps 04 & 06)
  users_daytype_file: "h3_hour_users_daytype.parquet"   # h3_cell, hour, daytype, n_users
  dwell_file: "h3_hour_dwell_median.parquet"    # h3_cell, hour, dwell_median_s
  exposure_file: "h3_hour_person_minutes.parquet"  # h3_cell, hour, person_minutes (context)
  kde_in_h3_file: "kde_in_h3.parquet"               # optional (from step 06)

  use_kde: true

  # Hours considered (UTC)
  hours_focus: [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23]

  # Weights for rank-normalised components (sum need not be 1; they are added)
#  weights:
#    h3_wd: 0.60
#    h3_we: 0.30
#    dwell: 0.10
#    kde:   0.00

  weights: {h3_wd: 0.55, h3_we: 0.25, dwell: 0.20, kde: 0.05}

#  weights:
#    h3_wd: 0.45
#    h3_we: 0.25
#    dwell: 0.30
#    kde: 0.00

  # Selection policy (per hour)
  top_percent: 70                     # head-of-list fraction before exclusions/dedup
  min_candidates_per_hour: 10
  max_candidates_per_hour: 5000
  min_distance_m_between: 0        # spacing between selected cells

  # Optional exclusions (e.g., parks, schools, residential-protected zones)
  exclusions_geojson: null           # e.g. "data/geo/exclusions.geojson"

  # Outputs
  out_parquet_pattern: "ads/candidates_{hour}.parquet"
  out_geojson_merged:  "geo/ads_candidates.geojson"

  # Logging
  log_file: "output/logs/ads_candidates.log"
  log_level: "INFO"


# -------------------------
# 08 – EVALUATION
# -------------------------
eval:
  # Files (relative to paths.parquet)
  points_file: "points_with_h3.parquet"           # used for hold-out ("from_points")
  users_file:  "h3_hour_users.parquet"            # fallback relevance ("from_users_file")
  candidates_pattern: "ads/candidates_{hour}.parquet"

  # Relevance construction
  heldout_mode: "from_points"     # "from_points" | "from_users_file"
  presence_threshold_s: 300       # must match Step 4 for a fair test
  test_days_frac: 0.30            # fraction of days used as held-out
  heldout_top_p: 0.10             # size of R_h per hour (top 10% of cells)
  random_seed: 13

  # Metrics
  hours: [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23]
  k: 25                         # evaluate P@k, R@k, F1@k
  stability_k: 25                 # daily top-k for Jaccard stability

  # Logging
  log_file: "output/logs/08_eval.log"
  log_level: "INFO"

eval_opt:
  # Relevance construction
  heldout_mode: from_points          # or "from_users_file" if raw points are unavailable
  presence_threshold_s: 300          # mirror Step 4
  heldout_top_p: 0.05                # top 5% per hour for |R_h|
  test_days_frac: 0.30
  hours: [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23]

  # Ranking metrics
  k: 50
  stability_k: 50

  # Candidates + inputs
  points_file: "points_with_h3.parquet"
  users_file: "h3_hour_users.parquet"
  candidates_pattern: "ads/candidates_{hour}.parquet"

  # Option 4: turn on multi-seed CI (comment this to run a single seed)
  multi_seeds: [13, 37, 101]         # 2–5 seeds are plenty
  # If multi_seeds is omitted, the script uses:
  random_seed: 13

  # Logging
  log_file: "output/logs/08_eval_opt.log"
  log_level: "INFO"
